{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.stats import entropy, itemfreq\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture as GMM\n",
    "from sklearn.metrics import adjusted_rand_score as ARI\n",
    "from sklearn.metrics import normalized_mutual_info_score as NMI\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS = {\n",
    "    \"pancreas\": {\"name\": \"pancreas\", \"batch_key\": \"study\", \"cell_type_key\": \"cell_type\",\n",
    "                 \"target\": [\"Pancreas SS2\", \"Pancreas CelSeq2\"]},\n",
    "    \"brain\": {\"name\": \"mouse_brain\", \"batch_key\": \"study\", \"cell_type_key\": \"cell_type\",\n",
    "              \"target\": [\"Tabula_muris\", \"Zeisel\"]}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering_scores(labels, newX, batch_ind):\n",
    "    n_labels = labels.nunique()\n",
    "    labels_pred = KMeans(n_labels, n_init=200).fit_predict(newX)\n",
    "    asw_score = silhouette_score(newX, batch_ind)\n",
    "    nmi_score = NMI(labels, labels_pred)\n",
    "    ari_score = ARI(labels, labels_pred)\n",
    "        \n",
    "    return asw_score, nmi_score, ari_score   \n",
    "\n",
    "def knn_purity(adata, label_key, n_neighbors=30):\n",
    "    labels = LabelEncoder().fit_transform(adata.obs[label_key].to_numpy())\n",
    "\n",
    "    nbrs = NearestNeighbors(n_neighbors=n_neighbors + 1).fit(adata.X)\n",
    "    indices = nbrs.kneighbors(adata.X, return_distance=False)[:, 1:]\n",
    "    neighbors_labels = np.vectorize(lambda i: labels[i])(indices)\n",
    "\n",
    "    # pre cell purity scores\n",
    "    scores = ((neighbors_labels - labels.reshape(-1, 1)) == 0).mean(axis=1)\n",
    "    res = [\n",
    "        np.mean(scores[labels == i]) for i in np.unique(labels)\n",
    "    ]  # per cell-type purity\n",
    "\n",
    "    return np.mean(res)\n",
    "\n",
    "def entropy_batch_mixing(latent, labels, n_neighbors=50, n_pools=50, n_samples_per_pool=100):\n",
    "    \n",
    "    def entropy_from_indices(indices):\n",
    "        return entropy(np.array(itemfreq(indices)[:, 1].astype(np.int32)))\n",
    "\n",
    "    neighbors = NearestNeighbors(n_neighbors=n_neighbors + 1).fit(latent)\n",
    "    indices = neighbors.kneighbors(latent, return_distance=False)[:, 1:]\n",
    "    batch_indices = np.vectorize(lambda i: labels[i])(indices)\n",
    "\n",
    "    entropies = np.apply_along_axis(entropy_from_indices, axis=1, arr=batch_indices)\n",
    "\n",
    "    # average n_pools entropy results where each result is an average of n_samples_per_pool random samples.\n",
    "    if n_pools == 1:\n",
    "        score = np.mean(entropies)\n",
    "    else:\n",
    "        score = np.mean([\n",
    "            np.mean(entropies[np.random.choice(len(entropies), size=n_samples_per_pool)])\n",
    "            for _ in range(n_pools)\n",
    "        ])    \n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.settings.autosave = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pancreas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = DATASETS['pancreas']\n",
    "data_name = data_dict['name']\n",
    "batch_key = data_dict['batch_key']\n",
    "cell_type_key = data_dict['cell_type_key']\n",
    "target_batches = data_dict['target']\n",
    "\n",
    "adata = sc.read(f\"./data/{data_name}_normalized.h5ad\")\n",
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target1, target2 = target_batches\n",
    "\n",
    "adata1 = adata[adata.obs[batch_key] == target1, :]\n",
    "adata2 = adata[adata.obs[batch_key] == target2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f\"./results/mnnCorrect/{data_name}/\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R\n",
    "\n",
    "library(scran)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch1 = pd.DataFrame(data=adata1.X.transpose(),\n",
    "                  index=adata1.var_names,\n",
    "                  columns=adata1.obs_names)\n",
    "\n",
    "batch2 = pd.DataFrame(data=adata2.X.transpose(),\n",
    "                  index=adata2.var_names,\n",
    "                  columns=adata2.obs_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i batch1 -i batch2 -i target1 -i target2\n",
    "\n",
    "for (i in 0:4){\n",
    "    for (subsample_frac in c(\"0.1\", \"0.2\", \"0.4\", \"0.6\", \"0.8\", \"1.0\")){\n",
    "        \n",
    "        keep_idx1 <- read.csv(paste(\"./data/subsample/\", data_name, \"/\", target1, \"/\", subsample_frac, \"/\", i, \".csv\", sep=\"\"), header = FALSE)[, 1] + 1\n",
    "        keep_idx2 <- read.csv(paste(\"./data/subsample/\", data_name, \"/\", target2, \"/\", subsample_frac, \"/\", i, \".csv\", sep=\"\"), header = FALSE)[, 1] + 1\n",
    "        \n",
    "        batch1_matrix <- data.matrix(batch1)[, keep_idx1]\n",
    "        batch2_matrix <- data.matrix(batch2)[, keep_idx2]\n",
    "        \n",
    "        mnnCorrected = mnnCorrect(batch1_matrix, batch2_matrix, BPPARAM=MulticoreParam(detectCores()))\n",
    "        \n",
    "        corrected1 = data.frame(mnnCorrected$corrected[[1]])\n",
    "        corrected2 = data.frame(mnnCorrected$corrected[[2]])\n",
    "\n",
    "        write.csv(corrected1, paste(\"./results/mnnCorrect/\", data_name, \"/corrected1-\", subsample_frac, \"-\", i, \".csv\", sep=\"\"))\n",
    "        write.csv(corrected2, paste(\"./results/mnnCorrect/\", data_name, \"/corrected2-\", subsample_frac, \"-\", i, \".csv\", sep=\"\"))\n",
    "        \n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    scores = []\n",
    "    for subsample_frac in [0.1, 0.2, 0.4, 0.6, 0.8, 1.0]:\n",
    "        final_adata = None\n",
    "        \n",
    "        keep_idx1 = np.loadtxt(f'./data/subsample/{data_name}/{target1}/{subsample_frac}/{i}.csv', dtype='int32')\n",
    "        adata_sampled1 = adata1[keep_idx1, :]\n",
    "        adata_sampled1.X = pd.read_csv(f'./results/mnnCorrect/{data_name}/corrected1-{subsample_frac}-{i}.csv', index_col=0).values.T\n",
    "        \n",
    "        keep_idx2 = np.loadtxt(f'./data/subsample/{data_name}/{target2}/{subsample_frac}/{i}.csv', dtype='int32')\n",
    "        adata_sampled2 = adata2[keep_idx2, :]\n",
    "        adata_sampled2.X = pd.read_csv(f'./results/mnnCorrect/{data_name}/corrected2-{subsample_frac}-{i}.csv', index_col=0).values.T\n",
    "        \n",
    "        final_adata = adata_sampled1.concatenate(adata_sampled2)\n",
    "        \n",
    "        \n",
    "        sc.tl.pca(final_adata, svd_solver=\"arpack\", n_comps=10)\n",
    "        final_adata = sc.AnnData(X=final_adata.obsm['X_pca'], obs=final_adata.obs)\n",
    "        \n",
    "        \n",
    "        asw_score, nmi_score, ari_score = clustering_scores(final_adata.obs[cell_type_key], final_adata.X, final_adata.obs[batch_key])\n",
    "        ebm_scores = []\n",
    "        for k in [15, 25, 50, 100, 200, 300]:\n",
    "            ebm_scores.append(entropy_batch_mixing(final_adata.X, final_adata.obs[batch_key], n_neighbors=k))\n",
    "            \n",
    "        knn_scores = []\n",
    "        for k in [15, 25, 50, 100, 200, 300]:\n",
    "            knn_scores.append(knn_purity(final_adata, label_key=cell_type_key, n_neighbors=k))\n",
    "            \n",
    "        scores.append([subsample_frac, asw_score, ari_score, nmi_score] + ebm_scores + knn_scores)\n",
    "        print(f\"{subsample_frac}-after\")\n",
    "        sc.pp.neighbors(final_adata)\n",
    "        sc.tl.umap(final_adata)\n",
    "        sc.settings.figdir = f\"./results/mnnCorrect/{data_name}/{i}/{subsample_frac}/after\"\n",
    "        sc.pl.umap(final_adata, color=[batch_key, cell_type_key], wspace=.5)\n",
    "        final_adata.write(f\"./results/mnnCorrect/{data_name}/{i}/{subsample_frac}/result_adata.h5ad\")\n",
    "\n",
    "    scores = np.array(scores)\n",
    "    np.savetxt(f\"./results/mnnCorrect/{data_name}/{i}.log\", X=scores, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mouse Brain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = DATASETS['brain']\n",
    "data_name = data_dict['name']\n",
    "batch_key = data_dict['batch_key']\n",
    "cell_type_key = data_dict['cell_type_key']\n",
    "target_batches = data_dict['target']\n",
    "\n",
    "adata = sc.read(f\"./data/{data_name}_normalized.h5ad\")\n",
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target1, target2 = target_batches\n",
    "\n",
    "adata1 = adata[adata.obs[batch_key] == target1, :]\n",
    "adata2 = adata[adata.obs[batch_key] == target2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f\"./results/mnnCorrect/{data_name}/\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch1 = pd.DataFrame(data=adata1.X.A.transpose(),\n",
    "                  index=adata1.var_names,\n",
    "                  columns=adata1.obs_names)\n",
    "\n",
    "batch2 = pd.DataFrame(data=adata2.X.A.transpose(),\n",
    "                  index=adata2.var_names,\n",
    "                  columns=adata2.obs_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i batch1 -i batch2 -i target1 -i target2\n",
    "\n",
    "for (i in 0:4){\n",
    "    for (subsample_frac in c(\"0.1\", \"0.2\", \"0.4\", \"0.6\", \"0.8\", \"1.0\")){\n",
    "        \n",
    "        keep_idx1 <- read.csv(paste(\"./data/subsample/\", data_name, \"/\", target1, \"/\", subsample_frac, \"/\", i, \".csv\", sep=\"\"), header = FALSE)[, 1] + 1\n",
    "        keep_idx2 <- read.csv(paste(\"./data/subsample/\", data_name, \"/\", target2, \"/\", subsample_frac, \"/\", i, \".csv\", sep=\"\"), header = FALSE)[, 1] + 1\n",
    "        \n",
    "        batch1_matrix <- data.matrix(batch1)[, keep_idx1]\n",
    "        batch2_matrix <- data.matrix(batch2)[, keep_idx2]\n",
    "        \n",
    "        mnnCorrected = mnnCorrect(batch1_matrix, batch2_matrix, BPPARAM=MulticoreParam(detectCores()))\n",
    "        \n",
    "        corrected1 = data.frame(mnnCorrected$corrected[[1]])\n",
    "        corrected2 = data.frame(mnnCorrected$corrected[[2]])\n",
    "\n",
    "        write.csv(corrected1, paste(\"./results/mnnCorrect/\", data_name, \"/corrected1-\", subsample_frac, \"-\", i, \".csv\", sep=\"\"))\n",
    "        write.csv(corrected2, paste(\"./results/mnnCorrect/\", data_name, \"/corrected2-\", subsample_frac, \"-\", i, \".csv\", sep=\"\"))\n",
    "        \n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    scores = []\n",
    "    for subsample_frac in [0.1, 0.2, 0.4, 0.6, 0.8, 1.0]:\n",
    "        final_adata = None\n",
    "        \n",
    "        keep_idx1 = np.loadtxt(f'./data/subsample/{data_name}/{target1}/{subsample_frac}/{i}.csv', dtype='int32')\n",
    "        adata_sampled1 = adata1[keep_idx1, :]\n",
    "        adata_sampled1.X = pd.read_csv(f'./results/mnnCorrect/{data_name}/corrected1-{subsample_frac}-{i}.csv', index_col=0).values.T\n",
    "        \n",
    "        keep_idx2 = np.loadtxt(f'./data/subsample/{data_name}/{target2}/{subsample_frac}/{i}.csv', dtype='int32')\n",
    "        adata_sampled2 = adata2[keep_idx2, :]\n",
    "        adata_sampled2.X = pd.read_csv(f'./results/mnnCorrect/{data_name}/corrected2-{subsample_frac}-{i}.csv', index_col=0).values.T\n",
    "        \n",
    "        final_adata = adata_sampled1.concatenate(adata_sampled2)\n",
    "        \n",
    "        \n",
    "        sc.tl.pca(final_adata, svd_solver=\"arpack\", n_comps=10)\n",
    "        final_adata = sc.AnnData(X=final_adata.obsm['X_pca'], obs=final_adata.obs)\n",
    "        \n",
    "        \n",
    "        asw_score, nmi_score, ari_score = clustering_scores(final_adata.obs[cell_type_key], final_adata.X, final_adata.obs[batch_key])\n",
    "        ebm_scores = []\n",
    "        for k in [15, 25, 50, 100, 200, 300]:\n",
    "            ebm_scores.append(entropy_batch_mixing(final_adata.X, final_adata.obs[batch_key], n_neighbors=k))\n",
    "            \n",
    "        knn_scores = []\n",
    "        for k in [15, 25, 50, 100, 200, 300]:\n",
    "            knn_scores.append(knn_purity(final_adata, label_key=cell_type_key, n_neighbors=k))\n",
    "            \n",
    "        scores.append([subsample_frac, asw_score, ari_score, nmi_score] + ebm_scores + knn_scores)\n",
    "        print(f\"{subsample_frac}-after\")\n",
    "        sc.pp.neighbors(final_adata)\n",
    "        sc.tl.umap(final_adata)\n",
    "        sc.settings.figdir = f\"./results/mnnCorrect/{data_name}/{i}/{subsample_frac}/after\"\n",
    "        sc.pl.umap(final_adata, color=[batch_key, cell_type_key], wspace=.5)\n",
    "        final_adata.write(f\"./results/mnnCorrect/{data_name}/{i}/{subsample_frac}/result_adata.h5ad\")\n",
    "\n",
    "    scores = np.array(scores)\n",
    "    np.savetxt(f\"./results/mnnCorrect/{data_name}/{i}.log\", X=scores, delimiter=\",\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
